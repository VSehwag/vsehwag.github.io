<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Why use synthetic data in machine learning?</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Nunito:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet"
    />
    <link
      rel="stylesheet"
      href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
      integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO"
      crossorigin="anonymous"
    />
    <script src="./../../src/jquery-3.5.1.min.js"></script>
    <script
      src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
      integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
      crossorigin="anonymous"
    ></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <!-- <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>‚úçüèæ</text></svg>" /> -->
    <link rel="stylesheet" type="text/css" href="./../../src/child.css" />
    <script src="./../../src/main.js"></script>
  </head>

  <body>
    <nav class="navbar navbar-expand-md">
      <div class="container">
        <a class="navbar-brand" href="./../../index.html">‚úçüèæ Sehwag's blog</a>
        <button
          class="navbar-toggler navbar-light"
          type="button"
          data-toggle="collapse"
          data-target="#main-navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="main-navigation">
          <ul class="navbar-nav">
            <li class="nav-item">
              <a class="nav-link" href="./../../../index.html">About</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <header class="container">
      <h1>
        Why we should be using synthetic data in (robust) machine learning
      </h1>
      <p>25 April 2022</p>
    </header>

    <div class="container content">
      <p>
        If you are excited about generative models, then you would find the
        latest improvements (e.g.,
        <a href="https://openai.com/dall-e-2/" target="_blank">DALLE-2</a> from
        OPENAI) nothing less than magical. These models excel at generating
        novel yet realistic synthetic images [<a
          href="https://arxiv.org/abs/2112.10741"
          target="_blank"
          >1</a
        >, <a href="https://arxiv.org/abs/2105.05233" target="_blank">2</a>,
        <a href="https://arxiv.org/abs/2202.00273" target="_blank">3</a>].
        Evident to this success are thousands of dalle-2 images floating on
        <a href="https://twitter.com/hashtag/dalle" target="_blank">twitter</a>,
        all of which are photorealistic synthetic images generated by this new
        model. Even before dalle-2, we have previous works that successfully
        solve this task on small scale but challenging datasets like ImageNet
        [<a href="https://arxiv.org/abs/2105.05233" target="_blank">1</a>,
        <a href="https://cascaded-diffusion.github.io/" target="_blank">2</a>].
        A large part of recent progress is fueled by work on diffusion models, a
        highly successful class of generative models [<a
          href="https://arxiv.org/pdf/2112.10752.pdf "
          target="_blank"
          >1</a
        >, <a href="https://arxiv.org/abs/2112.07804" target="_blank">2</a>,
        <a href="https://arxiv.org/abs/2112.05744" target="_blank">3</a>,
        <a href="https://arxiv.org/abs/2106.05931" target="_blank">4</a>].
      </p>

      <div class="w_100">
        <div class="gallery">
          <div class="figure" style="width: 20%">
            <img src="./data/rezende_synthetic.png" style="width: 100%" />
            <p class="caption" style="font-size: 0.7em">
              (a) Stochastic-Backprop (<a
                href="https://arxiv.org/pdf/1401.4082.pdf"
                >Rezende et al., 2014</a
              >)
            </p>
          </div>
          <div class="figure" style="width: 24%; margin-left: 30px">
            <img src="./data/goodfellow_synthetic.png" style="width: 100%" />
            <p class="caption" , style="font-size: 0.7em; width: 80%">
              (b) GAN (<a href="https://arxiv.org/abs/1406.2661"
                >Goodfellow et al., 2014</a
              >)
            </p>
          </div>
          <div class="figure" style="width: 25%; margin-left: 30px">
            <img src="./data/ddpm_synthetic.png" style="width: 100%" />
            <p class="caption" style="font-size: 0.7em; width: 80%">
              (c) Diffusion model (<a
                href="https://arxiv.org/pdf/2006.11239.pdf"
                >Ho et al., 2020</a
              >)
            </p>
          </div>
          <div class="figure" style="width: 22%">
            <img src="./data/dalle2_synthetic.png" style="width: 60%" />
            <p class="caption" style="font-size: 0.7em; width: 70%">
              (d) DALL¬∑E 2 (<a href="https://cdn.openai.com/papers/dall-e-2.pdf"
                >Ramesh et al., 2022</a
              >)
            </p>
          </div>
        </div>
        <div class="graphic">
          <p class="caption">
            Figure-1. <b>Progress in generative models.</b> Each subfigure
            presents synthetic/fake images from different generative models
            across years. While generative models have consistently made
            progress over years, recent class of diffusion based generative
            models finally brings the transformative capabilities of generative
            models to large, diverse, and challenging datasets (fig. c, d). For
            example, the dalle-2 (fig. d) generative model, which uses a
            diffusion model, can generate highly realistic images across a wide
            range of novel scenarios. In this post, I'll mainly discuss how we
            can improve representation learning using generative models, i.e.,
            by distilling knowledge embedded inside the generative model.
          </p>
        </div>
      </div>

      <p>
        When looking over the super realistic synthetic/fake images from modern
        generative model, a natural question to ask here is whether we can
        utilize these images to improve representation learning itself. Broadly
        speaking, can we transfer the knowledge embedded into generative models
        to downstream discriminative models. The most common usage of a similar
        phenomenon is in robotics and reinforcement learning, where an agent
        first learns to solve the task in a simulation and then transfers the
        learned knowledge to the real environment [<a
          href="https://lilianweng.github.io/posts/2019-05-05-domain-randomization/"
          target="_blank"
          >1</a
        >, <a href="https://ai.bu.edu/syn2real/" target="_blank">2</a>]. Here
        the simulator, built by us, acts as an approximate model of real world
        conditions. However, access to such a simulator is only accessible in
        certain scenarios (e.g., object manipulation by robots, autonomous
        driving). In contrast, a generative model learns an approximative model
        of real world using the training data. Thus for the task of having an
        approximation of real world model, it shifts the objective from
        <b>manual designing to learning it from data</b>, an approach that can
        democratize the synthetic-to-real learning approach. In other words,
        generative models can acts as <b>universal simulators</b>, due to
        applicability of their learning based approach to numerous applications. If
        we can transfer knowledge embedded in these generative models, then this
        approach has the potential to transform machine learning at broad scale.
      </p>

      <div class="w_100">
        <div class="gallery">
          <div class="figure" style="width: 23%">
            <img src="./data/conventional_learning.png" style="width: 100%" />
            <p class="caption">(a) Conventional learning from data</p>
          </div>
          <div class="figure" style="width: 23%">
            <img src="./data/simulator_learning.png" style="width: 100%" />
            <p class="caption">(b) Simulator assisted learning</p>
          </div>
          <div class="figure" style="width: 23%">
            <img src="./data/generative_learning.png" style="width: 100%" />
            <p class="caption">(c) Generative model assisted learning</p>
          </div>
        </div>
        <div class="graphic">
          <p class="caption">
            Figure-2.
            <b
              >Democratizing simulation-to-real learning with generative
              models.</b
            >
            When it comes to learning with generative models, one can draw
            similarities with well established <i>sim-to-real</i> approach [<a
              href="https://lilianweng.github.io/posts/2019-05-05-domain-randomization/"
              target="_blank"
              >1</a
            >, <a href="https://ai.bu.edu/syn2real/" target="_blank">2</a>]. In
            <i>sim-to-real</i> approach, we first learn representations in a
            simulated environment and utilize the acquired knowledge in real
            world tasks. We design such simulators to best immitate real world
            environment (e.g., in autonomous driving), but such simulators are
            only feasible for a handful of domains. In contrast, generative
            models can learn an approximation of real world environment from
            raw data. So the democratization comes from shifting the problem
            from <i>designing</i> an approximation of the real world environment to
            <i>learning</i> it from data, something deep learning is highly
            effictive at. The concrete research problem is how to best distill
            the knowledge from generative models for the downstream
            representation learning. As we will discuss next, the most
            straightforward way to incorporate generative models is by training
            on large amounts of synthetic images from it. Surprisingly this
            simple approach is highly effective in improving performance in
            common machine learning tasks.
          </p>
        </div>
      </div>

      <p>
        <b>Knowledge distillation from generative models.</b> The success of
        generative models assisted learning critically depends on how well we
        can distill the knowledge from it. We may even need customized methods
        for different generative models. For example, while GANs can
        be modified to learn unsupervised representations simultaneously with
        generative models (<a
          href="https://www.deepmind.com/open-source/bigbigan"
          target="_blank"
          >BigBiGAN</a
        >), such effective techniques haven't yet been developed for diffusion
        models. In contrast, a more intuitive and straightforward approach is
        applicable to all generative models: Extracting synthetic data from
        generative models and use it for training downstream models. We will
        consider this approach in most of our experiments in the post. While
        there is significant room to improve beyond it, it should serve as a
        common baseline for follow-up methods. I'll further discuss this
        direction at the end of the post.
      </p>

      <p></p>

      <p>
        Figure 3 summarizes our approach. We start with the images in the
        training dataset and train a generative model using them. An example
        would be training a DDPM (<a href="https://arxiv.org/abs/2006.11239"
          >Ho et al.</a
        >) or StyleGAN (<a href="https://arxiv.org/abs/2006.06676"
          >Karras et al.</a
        >) model using the 50,000 images in the cifar10 dataset. Once trained,
        the generative model gives us the ability to sample a very large number
        of synthetic images from it. In the following experiments, we will
        commonly sample one to ten million images. Finally, we combine the
        synthetic data with the real training images and train the classifier on
        the combined dataset. The hypothesis here is that the additional
        synthetic data will boost the performance of the classifier.
      </p>

      <div class="graphic w_90">
        <img src="./data/overview.jpeg" class="img-fluid" />
        <p class="caption">
          Figure-3. <b>Learning with synthetic data.</b> Overview of the
          pipeline where we distill knowledge from generative models by
          extracting a large amount of synthetic data from them and then using it in
          downstream representation learning.
        </p>
      </div>

      <h2>Part-1: Inflection point with progress in generative models</h2>
      <!-- <p>Lets consider a setup where we sample novel synthetic data from generatieve models and combine it with real training data. This is the most straightforward way to provide additional benefit of synthetic data in representation learning. But would it help?</p> -->
      Using synthetic data from generative models in training is pretty
      straightforward. <i>But would it help?</i>

      <div class="graphic w_90">
        <img
          src="./data/inflection_point.png"
          class="img-fluid"
          style="width: 75%"
        />
        <p class="caption">
          Figure-4.
          <b>Inflection point with progress in generative models.</b> Our
          objective is to measure how much additional boost we get in
          performance when we train a classifier on the combined set of real and
          synthetic data. At start of progress in generative models, low-quality
          synthetic data would likely degrade performance. An inflection point
          occurs when the synthetic data provides an additional boost in
          performance. On different datasets, we observe a varying degree of
          progress beyond the inflection point. E.g., we have progressed much
          farther on cifar10 than the ImageNet dataset.
        </p>
      </div>

      <p>
        <b>At start.</b> I would like to argue that it will likely demonstrate
        an <i>inflection point</i> with progress in the quality of synthetic
        data. In early stages of progress, the generative model will start to
        approximate real data distribution, but struggle to generate high
        fidelity images. E.g., synthetic images from some of the early work on
        GAN aren't highly photorealistic (fig. 1a, 1b). The synthetic data
        distribution learned by these early generative models will have a large
        gap from real data distribution, therefore simply combining these
        synthetic images with real data will lead to performance degradation.
        Note that this gap can be potentially minimized by using additional
        domain adaptation techniques [<a
          href="https://arxiv.org/pdf/1409.7495.pdf"
          target="_blank"
          >1</a
        >].
      </p>

      <p>
        <b>Near inflection point.</b> With improvement in generative models,
        one can start generating novel high fidelity images. Progress in
        generative models is a testament of it, where modern GANs generates
        stunning realistic image [<a
          href="https://nvlabs.github.io/stylegan3/"
          target="_blank"
          >1</a
        >, <a href="https://arxiv.org/pdf/2107.04589.pdf" target="_blank">2</a>,
        <a href="" target="_blank">3</a>], requiring dedicated efforts to
        distinguish them from the real ones, i.e., deepfake detection [<a
          href="https://arxiv.org/pdf/1812.08685.pdf"
          target="_blank"
          >1</a
        >,
        <a href="https://arxiv.org/pdf/2006.07397.pdf" target="_blank">2</a>].
        At this point, synthetic data certainly won't hurt the performance, since
        it lies very close to real-data distribution. But would it help, i.e., cross the inflection point?
      </p>

      <p>
        <b>Crossing inflection point.</b> To cross the inflection point, we not only
        need to generate novel high fidelity synthetic images but also achieve
        high diversity in these images. Synthetic images from GANs often lack
        diversity, which makes GANs not the most suitable choice. In contrast,
        diffusion based generative models achieve both high fidelity and
        diversity, simultaneously [<a
          href="https://arxiv.org/abs/2105.05233"
          target="_blank"
          >1</a
        >]. Across all datasets that we tested, we find that using synthetic
        images from diffusion models crosses the inflection point. Though how
        much it progresses beyond the inflection point varies across dataset. For
        example, while synthetic data from diffusion models bring a tremendous
        boost in performance on cifar10 dataset, it barely crosses the
        inflection point on ImageNet dataset.
      </p>

      <h3>
        State-of-the-art: Have we crossed the inflection point on common vision
        datasets?
      </h3>
      <p>
        The short answer, <b>yes</b>. We consider four datasets, namely cifar10,
        cifar100, imagenet, and celebA. For each dataset, we aim to train two
        networks. One trained on only real images and the other trained on
        combination of both real and synthetic images. If latter network
        achieves better test accuracy, then we claim that the synthetic data
        crosses the inflection point, i.e., using synthetic data boost
        performance.
      </p>

      <p>
        The next question is which experimental setup we should choose to study
        the impact of synthetic data. The first choice is baseline/benign
        training, i.e., training a neural network to achieve best generazation,
        i.e., test accuracy on images. However, we observe that synthetic data
        is even more helpful across a more challenging tasks, i.e., robust
        generalization (figure 5-a).
      </p>

      <h4>Curious case of robust/adversarial training</h4>
      The objective in adversarial/robust training is to harden the classifiers
      against adversarial examples (provide link). Thus the metric of interest
      is the accuracy on test-set adversarial examples, i.e., robust accuracy.
      Surprisingly, defending against adversarial examples is extremely hard.
      State-of-the-art robust accuracy, even on simpler dataset like cifar10,
      remain quite low. It is well established the generalization with
      adversarial training requires significantly more data [<a
        href="https://arxiv.org/abs/1804.11285"
        target="_blank"
        >1</a
      >]. This high sample complexity of adversarial training likely leads to
      the higher benefit of synthetic data.

      <div class="w_100">
        <div class="gallery">
          <div class="figure" style="width: 42%">
            <img src="./data/why_robust_training.png" class="img-fluid" />
            <p class="caption">
              (a) Success in benign and adversarial training.
            </p>
          </div>
          <div class="figure" style="width: 53%">
            <img
              src="./data/improvement_robust_accuracy.jpeg"
              class="img-fluid"
            />
            <p class="caption">
              (b) Success with different datasets in training with adversarial
              training.
            </p>
          </div>
        </div>
        <div class="graphic">
          <p class="caption">
            Figure-5. (a) <b>Why study robust training.</b> We first show that
            the impact of synthetic data is much more significant in robust training then
            the regular/benign training. This is particularly due to higher
            sample complexity of robust training. (b) We measure the benefit of
            training on both synthetic and real images, compared to just real
            images, on common image datasets.
          </p>
        </div>
      </div>

      <p>
        Across all four datasets, we find that training with combined real and
        synthetic data achieved better performance than training only on real
        data (Figure 5-b). However, the impact of synthetic data varies with
        datasets, e.g., in comparison to cifar10, the benefit on ImageNet is
        quite small. It brings us to the discussion on the inflection point, where
        the success of generative models varies across datasets. ImageNet is
        strictly a harder dataset than cifar (more number of classes, diverse
        images), making it much harder for generative models to generate both
        high quality and diverse images on this dataset.
      </p>

      <h2>
        Part-2: Understanding why synthetic helps
        <span style="font-size: 0.75em">(its not just about photorealism)</span>
      </h2>
      <p>
        The unique advantage of generative models is that we can sample
        unlimited amount of synthetic images from them. E.g., we used 1-10
        million synthetic images for most experiments. But as we highlighted in
        figure 4, augmenting synthetic images help only when progress in
        generative have cross an inflection point. Before we quantify the
        progress in this section, here is a challenge.
      </p>

      <div class="w_100">
        <div class="gallery">
          <div class="figure">
            <img src="./data/samples_images_real.png" />
            <p class="caption">(a) Real Images (CIFAR-10)</p>
          </div>
          <div class="figure">
            <img src="./data/samples_images_ddpm.png" />
            <p class="caption">
              (b)
              <a href="https://arxiv.org/abs/2006.11239" target="_blank"
                >DDPM</a
              >
            </p>
          </div>
          <div class="figure">
            <img src="./data/samples_images_styleC.png" />
            <p class="caption">
              (c)
              <a href="https://arxiv.org/abs/2006.06676" target="_blank"
                >StyleGAN</a
              >
            </p>
          </div>
        </div>
        <div class="graphic">
          <p class="caption">
            Figure-6. <b>Which generative model is better?</b> Can we identify
            which of the generative models (DDPM and StyleGAN) yields better
            quality synthetic images. We measure quality by the generalization
            accuracy on real images, i.e., when learning from synthetic data,
            how much accuracy we achieve on real data.
          </p>
        </div>
      </div>

      <p>
        In the figure above, we display real cifar-10 images for synthetic
        images from diffusion (DDPM) and stylegan based generative model. Our
        objective is to combine synthetic images from a generative models with
        real images in training cifar-10 classifier. Consider the following
        question:
        <b
          >Which of the two sets of synthetic images (DDPM vs StyleGAN) will be
          most helpful, when combined with real data?</b
        >
      </p>
      <p></p>
      <p>
        Both set of synthetic images are highly photo-realistic, but benefit of
        ddpm images significantly outperform styleGAN. For example, training
        with real+ddpm-synthetic images achieves more than 1-2% higher test
        accuracy than training on real+stylegan-synthetic images on cifar-10
        dataset. The difference is even higher than 5-6% with robust training.
        The motivation behind this question was to highlight the challenge of
        identifying the best generative model, even for humans. This is because
        the quality of synthetic data for purpose of represnetation learning
        depends on both image quality and diversity. While humans are an
        excellent judge of former, we need a distribution level comparison to
        concretely measure both.
      </p>

      <h4>
        How real is fake data? Measuring distinguishability of real and
        synthetic data distributions.
      </h4>
      <p>
        The common approach to measure the distribution distance between real
        and synthetic data using Fr√©chet inception distance (<a
          href="https://arxiv.org/abs/1706.08500"
          target="_blank"
          >FID</a
        >). FID simply measures the proximity of real and synthetic data using
        Wasserstein-2 distance in the feature space of deep neural network. So
        naturally the first approach would be to test if FID can explain why synthetic
        data from some generative models is more beneficial in learning than
        others. In particular, why diffusion models significantly more effective
        then contemporary GANs?
      </p>

      <p>
        To test this hypothesis, we consider six generative models on cifar10
        (five gans and one diffusion model). We first train a robust classifier
        on 1M synthetic images and measure the performance of real data. As
        expected, diffusion model synthetic images achieve much higher
        generalization than other generative models (Table 1). Next, we measure
        FID of synthetic images from each model. Surprisingly, FID doesn't align
        with the generalization performance observed when learning from
        synthetic data. E.g., FID for styleGAN is better than DDPM model while
        the latter achieves much better generalization performance on real data.
      </p>

      <p>
        Since the goal is to measure distinguishability of two distributions, we
        try a classification based approach. If synthetic data is
        indistinguishable from real, then it would be harder to classify them. 
        We test this hypothesis using a binary classifier. However, it
        turns out that even few layer neural network swere able successfully classify
        between real and synthetic data of all generative models with near 100%
        accuracy.
      </p>

      <div class="w_100">
        <div class="gallery">
          <div class="figure" style="width: 48%">
            <img src="./data/arc_motivation.jpeg" />
            <p class="caption">
              (a)
              <i>Binary classification beween real and synthetic data.</i> We
              expand each sample using \(\epsilon\)-ball, which makes
              classification success dependent on proximity of synthetic data
              to real data.
            </p>
          </div>
          <div class="figure" style="width: 43%">
            <img src="./data/arc_curve.png" />
            <p class="caption">
              (b) <i>ARC.</i> It refers to the area under the classification
              success and epsilon(\(\epsilon\)) curve. Lower ARC score implier
              higher proximity of synthetic data to real data.
            </p>
          </div>
        </div>
        <div class="graphic">
          <p class="caption">
            Figure-7. When using binary classification as a tool to measure
            the proximity between synthetic and real data, we encounter an unexpected issue. 
            Even a few layer network successfully classified all synthetic datasets from real. We introduce
            \(\epsilon\)-balls, i.e., expand each data point using an
            \(\epsilon\)-radius \(\ell_p\) ball around it and ask the classifier
            to classify these balls correctly. This simple trick makes the
            classification success dependent on proximity of real and fake data,
            since lower proximity will lead to balls intersections, thus making
            classification inpossible. One can then easily derive a metric (we
            name is ARC) which measures how hard the classification gets with
            increase in the size of \(\epsilon\)-balls.
          </p>
        </div>
      </div>

      <p>
        So we need to increase the dependence on discriminator success on
        distance between real and synthetic data distributions. This can be
        achieve using a very simple tool: \(\epsilon\)-balls (figure 7.a). We
        first draw a ball of radius \(r\) (it's a hypersphere if we use \(\ell_2\)
        norm and a hypercube for \(\ell_{\infty}\) norm) around each data point.
        Now the objective is to classify all \(\epsilon\)-balls correctly. If
        the synthetic and real dataset are in close proximity, drawing a
        decision boundary between them will become hard with small values of
        \(\epsilon\) itself. We measure the area under the classification
        success and \(\epsilon\) curve (referred to as ARC). ARC effectively
        measures the distinguishability of synthetic data from real data.
      </p>

      <p>
        ARC also explain why synthetic data from diffusion models is
        significantly more helpful than any other generative model. On cifar10
        dataset, ARC values for diffusion mode is 0.06, much lower than the best
        performing GAN (table-2). It also serves as a better metric than FID in
        predicting generative model success when their synthetic data is used in
        augment real data.
      </p>

      <div class="graphic">
        <img src="./data/arc_success.jpeg" class="img-fluid" />
        <p class="caption">
          Table-1.
          <b
            >Measuring distribution distance between real and synthetic data
            with ARC (Limitation of FID).</b
          >
          Our objective is to test whether the distance between real and synthetic
          datasets can predict the benefit of synthetic data in classification.
          For the ground truth, we adversarially train a wide-resnet model on
          one million synthetic images for each generative model and measure its
          robust accuracy on <i>real</i> cifar-10 images. Intuitively, if
          synthetic data is close to real data then we would expect it to provide
          higher benefit. But how do we measure proximity of synthetic data to
          real. FID is the most common metric for this task, where it measures
          the Wasserstein distance between real and synthetic data distribution
          in the feature space. However, models with better FID (lower is better)
          doesn't necessarily provide a better performance boost in learning.
          E.g., FID for styleGAN is better than diffusion model (ddpm) but the
          latter achieves better generalization on real data. As a solution, we
          propose ARC, which successfully explains the benefit of different
          generative models. Especially it explains why dffision models are much
          better than others since ARC score for diffusion models is much
          better than all other models in our study.
        </p>
      </div>

      <h3>Discussion</h3>
      <p>
        This post is largely based on my recent work that demonstrates benefit of
        synthetic data diffusion models in robust learning. The
        motivation to write it was to discuss the potential and bigger picture
        of how synthetic data can play a crucial role in deep learning,
        something that the rigid and scientific writing style of a paper doesn't
        permit. 
        <div class="bibtex">
          <pre>
Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?,
Sehwag et al., ICLR 2022 (<a href="https://arxiv.org/abs/2104.09425" target="_blank">Link</a>)
</pre
          >
        </div>
      </p>

      <p>
        Diffusion models finally enable the use of synthetic data as a mean to
        improve representation learning, i.e., move past the inflection point.
        With further progress in diffusion models, we will likely see higher
        utility from their synthetic data. However, one doesn't need to limit to
        using synthetic data as the only approach to integrate diffusion models
        in representation learning pipeline. In fact, the most important
        question in the research direction is
        <b>how to distill knowledge from diffusion models?</b> The current
        setup, i.e., sample synthetic images and use them with real data is a
        strong baseline, but has two limitations 1) It treats generative models
        in insolation to discriminative models 2) In addition, the generative
        models were trained without accounting that the resulted synthetic data
        will be used for augmentation in classification tasks. A more harmonious
        integration of both models will likely further improve performance.
      </p>

      <p>
        <b>Adaptive sampling. </b> Are are all synthetic samples equally
        beneficial? We touch upon this question in our work [<a href="https://arxiv.org/abs/2104.09425" target="_blank">1</a>] and show that
        one can get extra benefit from synthetic data by adaptively selecting
        samples. However, there is so much that can be done in this direction.
        Ideally we want to sample synthetic images from low-density regions on
        data manifold, i.e., regions on the data manifold that are poorly
        covered by real data.
      </p>

      <p>
        <b>Fine-grained metrics to measure synthetic data quality. </b> To
        develop adaptive sampling techniques, we essentially need to build
        measurement tools to indentify quality of different subgroups of
        synthetic images. In other words, what we can't measure, we can't
        understand. Metrics such as FID, Precision-Recall, and ARC only provides
        a distribution level measure of data quality. We would need to develop
        metric, or tune existing ones, to cater to sub-groups of our datsets.
      </p>
    </div>

    <script src="https://utteranc.es/client.js"
        repo="VSehwag/vsehwag.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>

    <!-- <footer class="page-footer">
      <div class="container"> -->
        <!-- <hr /> -->
      <!-- </div>
    </footer> -->
  </body>
  
</html>
