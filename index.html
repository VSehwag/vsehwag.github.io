<!DOCTYPE html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-178038666-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "UA-178038666-1");
  </script>

  <title>Vikash Sehwag - Academic webpage</title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link
    href="https://fonts.googleapis.com/css2?family=Nunito:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
    rel="stylesheet" />
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
    integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous" />
  <script src="./css/jquery-3.5.1.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css" />
  <script src="https://kit.fontawesome.com/b939870cfb.js" crossorigin="anonymous"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="shortcut icon" href="./images/favicon.ico" />
  <script src="./css/main.js"></script>
  <link rel="stylesheet" href="./css/main.css" />
  <base target="_blank">
</head>

<body>
  <nav class="navbar fixed-top navbar-expand-md">
    <div class="container">
      <a class="navbar-brand" target="_self" href="#"> Vikash Sehwag </a>
      <button class="navbar-toggler navbar-light" type="button" data-toggle="collapse" data-target="#main-navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="main-navigation">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" target="_self" href="#pubs">Publications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" target="_self" href="#code">Code</a>
          </li>
          <!-- <li class="nav-item">
            <a class="nav-link" href="#teaching">Teaching</a>
          </li> -->
          <li class="nav-item">
            <a class="nav-link" href="./docs/vikash_sehwag_cv.pdf" target="_blank">CV</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://vsehwag.github.io/blog">Blog</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <div class="full-project-divider top-divider"></div>

  <!-- Biography: first section of webpage -->
  <div class="container" id="aboutme">
    <div class="bio">
      <div class="box profile">
        <div class="headshot">
          <img src="./images/headshot.jpg" alt="Udari Madhushani" />
        </div>
        <h2 class="name section-heading"></h2>
        <h3 class="affil">Research Scientist, Sony AI</h3>
        <h3 class="email">sehwag.vikash@gmail.com</h3>
      </div>
      <div class="box about-me">
        <p>
          I am a research scientist at <a href="https://ai.sony/" target="_blank">Sony AI</a> where I lead efforts on enhancing safety and utility of large-scale generative models.
        </p>
        <p>
          I received my PhD from <a href="https://www.princeton.edu/">Princeton University</a> where I was advised by
          Prof. Prateek Mittal and Prof. Mung Chiang. I previously interned at <a href="https://ai.facebook.com/" target="_blank">Meta AI</a>  (AI Red Team) and
          <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/" target="_blank">Microsoft
            Research</a>. I
          have been fortunate to receive
          <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/winners"
            target="_blank">Qualcomm Innovation Fellowship</a> and the <a href="https://sites.google.com/view/advml/advml-rising-star-award">Rising Star Award</a> in adversarial machine learning. I previously organized the first
            <a href="https://vsehwag.github.io/SPML_seminar/" target="_blank">seminar series on Security & Privacy in
              Machine Learning (SPML)</a> at Princeton University.
        </p>
        <ul>
          <li>
            <a href="https://scholar.google.com/citations?user=JAkeEG8AAAAJ&hl=en" target="_blank"><i
                class="ai ai-google-scholar"></i></a>
          </li>
          <li>
            <a href="https://dblp.org/pid/187/5613.html" target="_blank"><i class="ai ai-dblp"></i></a>
          </li>
          <li>
            <a href="https://www.linkedin.com/in/vikash-sehwag/" target="_blank"><i class="fab fa-linkedin"></i></a>
          </li>
          <li>
            <a href="https://twitter.com/VSehwag_" target="_blank"><i class="fab fa-twitter"></i></a>
          </li>
          <li>
            <a href="https://github.com/VSehwag" target="_blank"><i class="fab fa-github"></i></a>
          </li>
        </ul>
      </div>
      <div class="interests">
        <div>
          <strong>Research Interests.</strong> Uncovering and mitigating safety risks in the development of next-generation of trustworthy AI systems.
          <ul>
            <li>
              <strong>Safer generative AI.</strong> We have demonstrated privacy risks in real-world diffusion models and developed privacy-preserving sampling and training methods [<a href="https://www.usenix.org/system/files/usenixsecurity23-carlini.pdf">1</a>, <a href="https://openreview.net/forum?id=vuVGcl0ed1">2</a>, <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/7058bc192a37f5e5a57398887b05f6f6-Abstract-Conference.html">3</a>, <a href="https://arxiv.org/abs/2212.04486">4</a>]. We have also developed techniques and benchmarked automated generation of adversarial and unsafe content from generative models [<a href="https://arxiv.org/abs/2203.17260">5</a>, <a href="./projects/2024/jbb/pl.html">6</a>].
            </li>
            <li>
              <strong>Responsible data synthesis.</strong> Concerned by the vast amount of generative content online, we've recently developed techniques to identify synthetic samples [<a href="./projects/2024/watermarks/pl.html">7</a>], even in the absence of artificial watermarks, and tracing them to source generative models [<a href="./projects/2024/tracing/pl.html">8</a>].
            </li>
            <li>
              <strong>Robust machine learning.</strong> We have conducted an in-depth exploration of adversarial robust learning, including circumventing its higher sample complexity using synthetic data [<a href="https://openreview.net/forum?id=WVX0NNVBBkV">9</a>], finding fundamental limits on robustness [<a href="https://proceedings.mlr.press/v139/bhagoji21a">10</a>], demonstrating higher robustness with transformers [<a href="https://openreview.net/forum?id=IztT98ky0cKs">11</a>], robustness across threat models [<a href="https://www.usenix.org/conference/usenixsecurity21/presentation/xiang">12</a>, <a href="https://proceedings.mlr.press/v202/wu23h/wu23h.pdf">13</a>, <a href="https://openreview.net/forum?id=SbAaNa97bzp">14</a>], the effect of model scaling and compression [<a href="https://arxiv.org/abs/2312.13131">15</a>, <a href="https://proceedings.neurips.cc/paper/2020/hash/e3a72c791a69f87b05ea7742e04430ed-Abstract.html">16</a>], and adversarial risks in transitioning from closed-domain to open-world systems [<a href="https://dl.acm.org/doi/10.1145/3338501.3357372">17</a>, <a href="https://openreview.net/pdf?id=v5gjXpmR8J">18</a>].
            </li>
            <li>
              <strong>Benchmarking progress in AI safety.</strong> We developed the widely adopted RobustBench benchmark [<a href="https://robustbench.github.io/">19</a>], followed by MultiRobustBench to account for multiple attacks [<a href="https://proceedings.mlr.press/v202/dai23c">20</a>], and most recently JailbreakBench [<a href="./projects/2024/jbb/pl.html">6</a>] to benchmark progress on jailbreaks against LLMs. We have also written a detailed discussion on nuanced similarity and distinction in security and safety approaches towards Trustworthy AI [<a href="./projects/2024/sec_vs_safety/pl.html">21</a>].
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
  <!-- News: Second section of the webpage-->
  <!-- <div class="news container">
    <div class="news-mini-container">
      <div class="news-grid">
        <p class="section-heading">News</p>
        <div class="flex-container">
          <div>02/2023</div>
          <div class="news-text">
            New paper on extracting training data from diffusion models (<a
              href="https://arxiv.org/abs/2301.13188">pdf</a>)
          </div>
        </div>
        <div class="flex-container">
          <div>12/2022</div>
          <div class="news-text">
            Presented <a href="https://arxiv.org/abs/2206.09868">our paper</a> on understanding robust representations
            at Neurips'22.
          </div>
        </div>
        <div class="flex-container">
          <div>09/2022</div>
          <div class="news-text">
            Awarded graduate student award for excellence in service (ECE, Princeton University).
          </div>
        </div>
        <div class="flex-container">
          <div>04/2022</div>
          <div class="news-text">
            Awarded <a
              href="https://ece.princeton.edu/news/honorific-fellowships-fund-research-quantum-sensors-and-trustworthy-ai"
              target="_blank">Charlotte Elizabeth Proctor Honorific Fellowship</a>, one of the highest honors at
            Princeton University.
          </div>
        </div>
        <div class="flex-container">
          <div>03/2022</div>
          <div class="news-text">
            Paper on low-density sampling from diffusion models got accepted at
            <a href="https://arxiv.org/abs/2203.17260" target="_blank">CVPR'22</a>.
          </div>
        </div>
        <div class="flex-container">
          <div>01/2022</div>
          <div class="news-text">
            Paper on using synthetic data in robust learning got accepted at
            <a href="https://arxiv.org/abs/2104.09425" target="_blank">ICLR'22</a>.
          </div>
        </div>
        <div class="flex-container">
          <div>08/2021</div>
          <div class="news-text">
            Finished an amazing research internship at
            <a href="https://ai.facebook.com/">Facebook AI</a>.
          </div>
        </div>
        <div class="flex-container">
          <div>05/2021</div>
          <div class="news-text">
            Paper on lower bounds on adversarial robustness accepted at
            <a href="https://icml.cc/">ICML 2021 </a> (<a href="https://arxiv.org/pdf/2104.08382.pdf">pdf</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>04/2021</div>
          <div class="news-text">
            Paper on improving robustness using proxy distributions is now out
            (<a href="https://arxiv.org/abs/2104.09425">pdf</a>)!
          </div>
        </div>

        <div class="flex-container">
          <div>03/2021</div>
          <div class="news-text">
            <a href="https://robustbench.github.io/">RobustBench</a> won best
            paper honorable mention prize at
            <a href="https://aisecure-workshop.github.io/aml-iclr2021/cfp">ICLR AiSecure</a>
            workshop.
          </div>
        </div>
        <div class="flex-container">
          <div>01/2021</div>
          <div class="news-text">
            Self-supervised outlier detection (SSD) paper accepted at
            <a href="https://iclr.cc/Conferences/2021/Dates" target="_blank">ICLR 2021</a>
            (<a href="https://openreview.net/pdf?id=v5gjXpmR8J" target="_blank">pdf</a>,
            <a
              href="https://slideslive.com/38954089/ssd-a-unified-framework-for-selfsupervised-outlier-detection?ref=search">slides</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>01/2021</div>
          <div class="news-text">
            Paper on PatchGuard accepted at
            <a href="https://www.usenix.org/conference/usenixsecurity21">USENIX Security</a>
            2021 (<a href="https://www.usenix.org/system/files/sec21fall-xiang.pdf">pdf</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>10/2020</div>
          <div class="news-text">
            Releasing
            <a href="https://robustbench.github.io" target="_blank">RobustBench</a>, a standardized benchmark for
            adversarial robustness.
          </div>
        </div>
        <div class="flex-container">
          <div>10/2020</div>
          <div class="news-text">
            Work on fast-convergent federated learning to appear in
            <a href="https://www.comsoc.org/publications/journals/ieee-jsac">IEEE JSAC</a>
            (<a href="https://arxiv.org/abs/2007.13137">arxiv</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>09/2020</div>
          <div class="news-text">
            Paper on prning robust networks (Hydra) accepted at
            <a href="https://neurips.cc/">NeurIPS 2020</a>. (<a href="https://vsehwag.github.io/hydra/">webpage</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>07/2020</div>
          <div class="news-text">
            Paper on background check of deep learning - ICML OOL workshop (<a
              href="https://arxiv.org/pdf/2006.14077.pdf">pdf</a>,
            <a href="https://oolworkshop.github.io/program/ool_26.html">video</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>07/2020</div>
          <div class="news-text">
            Work on separability of self-supervised representations, and another
            one on critical evaluation of open-world meachine learning, accepted
            at ICML UDL workshop.
          </div>
        </div>
        <div class="flex-container">
          <div>06/2020</div>
          <div class="news-text">
            Volunteered as junior mentor at
            <a href="https://researchcomputing.princeton.edu/events/princeton-olcf-nvidia-gpu-hackathon">Princeton-OLCF-NVIDIA
              GPU Hackathon.</a>
          </div>
        </div>
        <div class="flex-container">
          <div>05/2020</div>
          <div class="news-text">
            Releasing PatchGuard, a provable defense against adversarial patches
            (<a href="https://arxiv.org/pdf/2005.10884.pdf">Pdf</a>,
            <a href="https://github.com/inspire-group/PatchGuard">Code</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>04/2020</div>
          <div class="news-text">
            Work on pruning robust networks accepted at
            <a href="https://trustworthyiclr20.github.io/" target="_blank">ICLR TTML workshop</a>
            (<a
              href="https://docs.google.com/presentation/d/1yLSnvUR5MFhv-Dp2yuQEF1QuFNffTSomUGuiZsZJmbY/edit?usp=sharing">slides</a>,
            <a
              href="https://slideslive.com/38926541/on-pruning-adversarially-robust-neural-networks?ref=account-folder-46630-folders">video</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>01/2020</div>
          <div class="news-text">
            Taught a mini-course on adversarial attacks & defenses in
            Winterssion 2020 (<a
              href="https://docs.google.com/presentation/d/1bs7xMYndjUshWkgLppmpnle_ZWkVTA7QoOfJ79nG-Ko/edit?usp=sharing">Slides</a>,
            <a
              href="https://colab.research.google.com/drive/1Pyn8zgZUlBKz18kSL0vO0ojBE3AnVpl6?usp=sharing">Colab-notebook</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>09/2019</div>
          <div class="news-text">
            Finished amazing summer research internship at Microsoft Research,
            Redmond.
          </div>
        </div>
        <div class="flex-container">
          <div>08/2019</div>
          <div class="news-text">
            Paper on robust open-world machine learning accepted at
            <a href="https://aisec.cc/">AISec 2019</a> (<a
              href="https://docs.google.com/presentation/d/12NQfyzcztr00gjicu0-BzROV178_fSJNTXMraoCspvw/edit?usp=sharing">Slides</a>).
          </div>
        </div>
        <div class="flex-container">
          <div>05/2019</div>
          <div class="news-text">
            Won
            <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/winners">Qualcomm
              Innovation Fellowship 2019</a>.
          </div>
        </div>
      </div>
    </div>
  </div> -->
  
  <!-- Publications: The major section in the middle of website-->
  <div class="publications container" id="pubs">
    <!-- <div class="publications container" id="pubs"> -->
      <div class="full-project-divider2"><hr /></div>
    <p class="section-heading">Publications</p>
    <!-- Create a copy of pubs-box for a new entry and pubs-section for a group of entries-->
    <h5>2024</h5>

    <div class="pubs-box key-pubs">
      <div class="pubs-image">
        <img src="./projects/trace/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          How to Trace Latent Generative Model Generated Images without Artificial Watermark?
        </p>
        <p class="authors">
          Zhenting Wang, <em>Vikash Sehwag</em>, Chen Chen, Lingjuan Lyu, Dimitris N. Metaxas, Shiqing Ma
        </p>
        <p>
          <span class="venue">ICML 2024 (to appear)</span>
          <!-- <span class="links">
            <a href="https://openreview.net/pdf?id=TwZ2sY6eJj" target="_blank">pdf</a> | 
            <a href="https://github.com/inspire-group/DP-RandP" target="_blank">code</a>
          </span> -->
        </p>
        <p class="summary">
          Using signature from the latent autoencoders, we propose an approach to trace synthetic images back to the source latent generative model.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/linearsc/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization
        </p>
        <p class="authors">
          Ashwinee Panda, Xinyu Tang, <em>Vikash Sehwag</em>, Saeed Mahloujifar, Prateek Mittal
        </p>
        <p>
          <span class="venue">ICML 2024 (to appear)</span>
        </p>
        <p class="summary">
          We consider the cost of hyperparameter optimization in differentially private learning and propose a strategy that prvoides linear scaling of hyperparameters, thus reducing the privacy cost and simultaneously achieving state-of-the-art performance across 22 benchmark tasks in CV and NLP. 
        </p>
      </div>
    </div>

    <div class="pubs-box key-pubs">
      <div class="pubs-image">
        <img src="./projects/jbb/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models
        </p>
        <p class="authors">
          Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, <em>Vikash Sehwag</em>, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, Eric Wong
        </p>
        <p>
          <span class="venue">Arxiv 2024 - </span>
          <span class="links">
            <a href="https://arxiv.org/abs/2404.01318" target="_blank">pdf</a> | 
            <a href="https://jailbreakbench.github.io/" target="_blank">webpage</a> |
            <a href="https://github.com/JailbreakBench/jailbreakbench" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          A centralized benchmark for 1) repository of jailbreaking attacks and artifacts, 2) standardized evaluation framework, and 3) up-to-date leaderboard. 
        </p>
      </div>
    </div>
    
    <h4>2023</h4>
    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/dprandp/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          Differentially Private Image Classification by Learning Priors from Random Processes
        </p>
        <p class="authors">
          Xinyu Tang, Ashwinee Panda, <em>Vikash Sehwag</em>, Prateek Mittal
        </p>
        <p>
          <span class="venue">NeurIPS 2023 (<span class="redtext">spotlight</span>) - </span>
          <span class="links">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/7058bc192a37f5e5a57398887b05f6f6-Paper-Conference.pdf" target="_blank">pdf</a> | 
            <a href="https://github.com/inspire-group/DP-RandP" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          We show that pre-training on data from random processes enables better performance during differentially private finetuning, while simultaneously avoiding privacy leakage associated with real pretraining images. 
        </p>
      </div>
    </div>

    <div class="pubs-box key-pubs">
      <div class="pubs-image">
        <img src="./projects/memorize/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          Extracting Training Data from Diffusion Models
        </p>
        <p class="authors">
          Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, <em>Vikash Sehwag</em>, Florian Tram√®r, Borja
          Balle, Daphne Ippolito, Eric Wallace
        </p>
        <p>
        <span class="venue">USENIX Security Symposium, 2023 - </span>
        <span class="links"><a href="https://arxiv.org/abs/2301.13188" target="_blank">pdf</a> | <a href="https://www.youtube.com/watch?v=KyST9QH7VfE&ab_channel=USENIX">video</a> | <a href="#" onclick="return false;">News</a> (<a href="https://www.technologyreview.com/2023/02/03/1067786/ai-models-spit-out-photos-of-real-people-and-copyrighted-images/">1</a>, <a href="https://arstechnica.com/information-technology/2023/02/researchers-extract-training-images-from-stable-diffusion-but-its-difficult/">2</a>, <a href="https://www.vice.com/en/article/m7gznn/ai-spits-out-exact-copies-of-training-images-real-people-logos-researchers-find">3</a>, <a href="https://gizmodo.com/ai-art-generators-ai-copyright-stable-diffusion-1850060656#:~:text=Researchers%20in%20both%20industry%20and,to%20simply%20reproduce%20an%20image.">4</a>)</span>
        </p>
        <p class="summary">
          This was one of the first works to demonstrate significant memorization of real-world images in web-scale text-to-image generative models (Stable Diffusion, ImageN). Our findings further motivated web-scale data deduplication in training dataset of generative models.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/tta/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          Uncovering Adversarial Risks of Test-Time Adaptation
        </p>
        <p class="authors">
          Tong Wu, Feiran Jia, Xiangyu Qi, Jiachen T. Wang, <em>Vikash Sehwag</em>, Saeed Mahloujifar, Prateek Mittal
        </p>
        <p>
          <span class="venue">ICML 2023 - </span>
          <span class="links">
            <a href="https://proceedings.mlr.press/v202/wu23h/wu23h.pdf" target="_blank">pdf</a> | 
            <a href="https://tongwu2020.github.io/tongwu/tta_risk/index.html" target="_blank">webpage</a> |
            <a href="https://github.com/inspire-group/tta_risk" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          We show that test-time adaptation, a technique that aims to improve performance at test time, also increases exposure to novel security risks.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/multirobust/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          MultiRobustBench: Benchmarking Robustness Against Multiple Attacks
        </p>
        <p class="authors">
          Sihui Dai, Saeed Mahloujifar, Chong Xiang, <em>Vikash Sehwag</em>, Pin-Yu Chen, Prateek Mittal
        </p>
        <p>
          <span class="venue">ICML 2023 - </span>
          <span class="links">
            <a href="hhttps://proceedings.mlr.press/v202/dai23c/dai23c.pdf" target="_blank">pdf</a> | 
            <a href="https://multirobustbench.github.io/" target="_blank">webpage</a> |
            <a href="https://github.com/multirobustbench/multirobustbench" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          Going beyond single-attack robustness (RobustBench), we develop a standardized benchmark for multi-attack threat vectors.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/lightVits/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          A Light Recipe to Train Robust Vision Transformers
        </p>
        <p class="authors">
          Edoardo Debenedetti, <em>Vikash Sehwag</em>, Prateek Mittal
        </p>
        <p>
          <span class="venue">SaTML 2023 - </span>
          <span class="links">
            <a href="https://arxiv.org/abs/2209.07399" target="_blank">pdf</a> | 
            <a href="https://www.youtube.com/watch?v=2UY85XlJcHY&ab_channel=NicolasPapernot" target="_blank">video</a> | 
            <a href="https://edoardo.science/publication/light_recipe/slides.pdf" target="_blank">slides</a> | 
            <a href="https://github.com/dedeswim/vits-robustness-torch" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          Contrary to the conventional wisdom of using heavy data augmentation in ViTs, we showed that a lighter data
          augmentation (along with other bag-of-tricks) achieves state-of-the-art performance with ViTs adversarial training.
        </p>
      </div>
    </div>

    <h4>2022</h4>
    <div class="pubs-box key-pubs">
      <div class="pubs-image">
        <img src="./projects/ab_sampling/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          Generating High Fidelity Data from Low-density Regions using Diffusion Models
        </p>
        <p class="authors">
          <em>Vikash Sehwag</em>, Caner Hazirbas, Albert Gordo, Firat Ozgenel,
          Cristian Canton Ferrer
        </p>
        <p>
          <span class="venue">CVPR 2022 - </span>
          <span class="links">
            <a href="https://arxiv.org/abs/2203.17260" target="_blank">pdf</a>
          </span>
        </p>
        <p class="summary">
          Our work showed strong generalization of diffusion models in the tail of the data distribution and developed adaptive sampling techniques to generate high-fidelity samples from the tail of the data distribution.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/cka/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          Understanding Robust Learning through the Lens of Representation Similarities
        </p>
        <p class="authors">
          Christian Cianfarani, Arjun Nitin Bhagoji, <em>Vikash Sehwag</em>, Ben Y. Zhao, Prateek Mittal, Haitao Zheng
        </p>
        <p>
          <span class="venue">NeurIPS 2022 - </span>
          <span class="links">
            <a href="https://arxiv.org/abs/2206.09868" target="_blank">pdf</a> | 
            <a href="https://slideslive.com/embed/presentation/38992183" target="_blank">video</a> | 
            <a href="https://slideslive.com/embed/presentation/38992183" target="_blank">slides</a> | 
            <a href="https://github.com/inspire-group/robust_representation_similarity" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          Using representation similarity metrics, such as CKA, we demonstrate multiple interesting characteristics of
          adversarially robust networks compared to non-robust networks.
        </p>
      </div>
    </div>

    <div class="pubs-box key-pubs">
      <div class="pubs-image">
        <img src="./projects/proxy/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?
        </p>
        <p class="authors">
          <em>Vikash Sehwag</em>, Saeed Mahloujifar, Tinashe Handina, Sihui Dai,
          Chong Xiang, Mung Chiang, Prateek Mittal
        </p>
        <p>
          <span class="venue">ICLR 2022 - </span>
          <span class="links">
            <a href="https://arxiv.org/abs/2104.09425" target="_blank">pdf</a> | 
            <a href="https://iclr.cc/virtual/2022/poster/6374" target="_blank">video</a> | 
            <a href="https://iclr.cc/virtual/2022/poster/6374" target="_blank">slides</a> | 
            <a href="https://github.com/inspire-group/proxy-distributions" target="_blank">code</a> |
            <a href="https://vsehwag.github.io/blog/2022/4/synthetic_data_in_ml.html" target="_blank">blog</a>
          </span>
        </p>
        <p class="summary">
          We showed that synthetic data from diffusion model provides a termendous boost in the generalization performance of adversarial training.
        </p>
      </div>
    </div>

    <h4>2021</h4>
    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/ce_bounds/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          Lower Bounds on Cross-Entropy Loss in the Presence of Test-time Adversaries
        </p>
        <p class="authors"> Arjun Nitin Bhagoji, Daniel Cullina, <em>Vikash Sehwag</em>, Prateek Mittal </p>
        <p>
          <span class="venue">ICML 2021 - </span>
          <span class="links">
            <a href="https://arxiv.org/abs/2203.17260" target="_blank">pdf</a> | 
            <a href="https://slideslive.com/38959167/lower-bounds-on-crossentropy-loss-in-the-presence-of-testtime-adversaries?ref=speaker-18989" target="_blank">video</a> | 
            <a href="https://slideslive.com/38959167/lower-bounds-on-crossentropy-loss-in-the-presence-of-testtime-adversaries?ref=speaker-18989" target="_blank">slides</a> | 
            <a href="https://arjunbhagoji.github.io/files/poster_icml_2021.pdf" target="_blank">poster</a> |
            <a href="https://github.com/arjunbhagoji/log-loss-lower-bounds" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          We provide lower-bounds on cross-entropy loss in the persence of adversarial attacks for common small-scale computer vision datasets.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/ssd/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          SSD: A Unified Framework for Self-Supervised outlier detection
        </p>
        <p class="authors">
          <em>Vikash Sehwag</em>, Mung Chiang, Prateek Mittal
        </p>
        <p>
          <span class="venue">ICLR 2021, NeurIPS SSL workshop 2020 - </span>
          <span class="links">
            <a href="https://arxiv.org/abs/2103.12051" target="_blank">pdf</a> | 
            <a href="https://slideslive.com/38954089/ssd-a-unified-framework-for-selfsupervised-outlier-detection?ref=search" target="_blank">video</a> | 
            <a href="https://slideslive.com/38954089/ssd-a-unified-framework-for-selfsupervised-outlier-detection?ref=search" target="_blank">slides</a> | 
            <a href="https://github.com/inspire-group/SSD" target="_blank">code</a>
          </span>
        </p>

        <p class="summary">
          Using <em>only</em> unlabeled data, we develop a highly succesful
          framework to detect outliers/out-of-distribution samples.
        </p>
      </div>
    </div>

    <!-- <div class="pub-divider">
      <hr />
    </div> -->

    <div class="pubs-box key-pubs">
      <div class="pubs-image">
        <img src="./projects/robustbench/logo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          RobustBench: A Standardized Adversarial Robustness Benchmark
        </p>
        <p class="authors">
          Francesco Croce, Maksym Andriushchenko, <em>Vikash Sehwag</em>, Nicolas Flammarion, Mung Chiang, Prateek Mittal, Matthias Hein
        </p>
        <p>
          <span class="venue">NeurIPS 2021 - </span>
          <span class="links">
            <a href="https://robustbench.github.io/" target="_blank">leaderboard</a> | 
            <a href="https://arxiv.org/abs/2010.09670" target="_blank">pdf</a> | 
            <a href="https://github.com/RobustBench/robustbench" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          We develop a standardized benchmark to track progress on adversarial robustness in deep learning. Our benchmark has been highly insightful and been visited by more than 40K users.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/patchguard/demo.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          PatchGuard: Provable Defense against Adversarial Patches Using Masks on Small Receptive Fields
        </p>
        <p class="authors">
          Chong Xiang, Arjun Nitin Bhagoji, <em>Vikash Sehwag</em>, Prateek Mittal
        </p>
        <p>
          <span class="venue">USENIX Security Symposium 2021 - </span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2005.10884.pdf" target="_blank">pdf</a> | 
            <a
            href="https://www.youtube.com/watch?v=t20yGBIdZ20&ab_channel=USENIX"
            target="_blank">video</a> |
            <a href="https://github.com/inspire-group/PatchGuard" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          A general defense framework to acheive provable robustness against adversrial patches.
        </p>
      </div>
    </div>

    <!-- <div class="pub-divider">
      <hr />
    </div> -->

    <h4>2020</h4>
    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/hydra/hydra.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          HYDRA: Pruning Adversarially Robust Neural Networks
        </p>
        <p class="authors">
          <em>Vikash Sehwag</em>, Shiqi Wang, Prateek Mittal, Suman Jana
        </p>
        <p>
          <span class="venue">NeurIPS 2020 - </span>
          <span class="links">
            <a href="https://vsehwag.github.io/hydra" target="_blank">webpage</a> | 
            <a href="https://arxiv.org/abs/2002.10509" target="_blank">pdf</a> | 
            <a
            href="https://slideslive.com/38926541/on-pruning-adversarially-robust-neural-networks?ref=account-folder-46630-folders"
            target="_blank">video</a> |
            <a
            href="https://slideslive.com/38926541/on-pruning-adversarially-robust-neural-networks?ref=account-folder-46630-folders"
            target="_blank">slides</a> |
            <a href="https://github.com/inspire-group/compactness-robustness" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          We achieved state-of-the-art clean and robust accuracy when aggressively pruning the parameters of deep neural networks.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/folb/system.png" />
      </div>
      <div class="pubs-data">
        <p class="title">Fast-Convergent Federated Learning</p>
        <p class="authors">
          Hung T. Nguyen, <em>Vikash Sehwag</em>, Seyyedali Hosseinalipour,
          Christopher G. Brinton, Mung Chiang, H. Vincent Poor
        </p>
        <p>
          <span class="venue">IEEE Journal on Selected Areas in Communications (J-SAC) - Series on Machine Learning for Communications and Networks 2020 - </span>
          <span class="links">
            <a href="https://arxiv.org/abs/2007.13137" target="_blank">pdf</a>
          </span>
        </p>
        <p class="summary">
          We proposed a fast-convergent federated learning algorithm, called
          FOLB, which improves convergence speed by an smart sampling of
          devices in each round.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image"><img src="./projects/ood_udl/tsne.png" /></div>
      <div class="pubs-data">
        <p class="title">
          A Critical Evaluation of Open-World Machine Learning
        </p>
        <p class="authors">
          Liwei Song, <em>Vikash Sehwag</em>, Arjun Nitin Bhagoji, Prateek
          Mittal
        </p>
        <p>
          <span class="venue">ICML Workshop on Uncertainty & Robustness 2020 - </span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2007.04391.pdf" target="_blank">pdf</a> |
            <a href="https://github.com/inspire-group/OOD-Attacks" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          We demonstrate a fundamental conflict between the learning objectives of open-world machine
          learning and adversarial robustness.
        </p>
      </div>
    </div>

    <div class="pubs-box">
      <div class="pubs-image">
        <img src="./projects/ood_aisec/outline.png" />
      </div>
      <div class="pubs-data">
        <p class="title">
          Analyzing the Robustness of Open-World Machine Learning
        </p>
        <p class="authors">
          <em>Vikash Sehwag</em>, Arjun Nitin Bhagoji, Liwei Song, Chawin
          Sitawarin, Daniel Cullina, Mung Chiang, Prateek Mittal
        </p>
        <p>
          <span class="venue">ACM Workshop on Artificial Intelligence and Security (AISec) 2019 - </span>
          <span class="links">
            <a href="https://dl.acm.org/doi/pdf/10.1145/3338501.3357372" target="_blank">pdf</a> |
            <a href="https://docs.google.com/presentation/d/12NQfyzcztr00gjicu0-BzROV178_fSJNTXMraoCspvw/edit?usp=sharing" target="_blank">slides</a> |
            <a href="https://github.com/inspire-group/OOD-Attacks" target="_blank">code</a>
          </span>
        </p>
        <p class="summary">
          We demonstrate the vulnerability of open-world machine learning models to adversarial
          examples and proposed a defense against the open-world adversarial attacks. 
        </p>
      </div>
    </div>
    
    <div class="full-project-divider2">
      <hr />
    </div>

    <div class="news container" id="code">
      <p class="section-heading-small">Selected Open Source Repositories</p>
      <div class="pubs-section work-in-progress">
        <ul class="simple-ord-list">
          <li><a href="https://github.com/RobustBench/robustbench">https://github.com/RobustBench/robustbench</a> | &#9733; 598 | <em>RobustBench leaderboard</em></li>
          <li><a href="https://github.com/VSehwag/minimal-diffusion">https://github.com/VSehwag/minimal-diffusion</a> | &#9733; 214 | <em>Minimalistic implementation of diffusion models</em></li>
          <li><a href="https://github.com/inspire-group/SSD">https://github.com/inspire-group/SSD</a> | &#9733; 125 | <em>Self-supervised outlier detection</em></li>
          <li><a href="https://github.com/inspire-group/hydra">https://github.com/inspire-group/hydra</a> | &#9733; 88 | <em>Pruning adversarial robust networks</em></li>
          <li><a href="https://github.com/inspire-group/proxy-distributions">https://github.com/inspire-group/proxy-distributions</a> | &#9733; 26 | <em>Improving adversarial robustness using synthetic data</em></li>
          <li><a href="https://github.com/inspire-group/OOD-Attacks">https://github.com/inspire-group/OOD-Attacks</a> | &#9733; 12 | <em>Robust open-world machine learning</em></li>
          <li><a href="https://github.com/inspire-group/robust_representation_similarity">https://github.com/inspire-group/robust_representation_similarity</a> | &#9733; 6 | <em>Representation similarity analysis for robust and non-robust networks</em></li>
        </ul>
    </div>

    <div class="news container" id="code">
      <p class="section-heading-small">Invited Talks</p>
      <div class="pubs-section work-in-progress">
        <ul class="simple-ord-list">
          <li>On safety risks of generative AI - From ChatGPT to DallE.3 | <em>Nov 2023</em> | <em>Columbia University</em></li>
          <li>Prospects and pitfalls of modern generative models - An AI safety perspective  | <em>Feb 2023</em> | <em>AAAI</em></li>
          <li>Enhancing machine learning using synthetic data distilled from generative models | <em>Jan 2023</em> | <em>MSR</em></li>
          <li>Role of synthetic data in trustworthy machine learning | <em>May 2022</em> | <em>UChicago, UBerkeley</em></li>
          <li>A generative approach to robust machine learning | <em>Mar 2022</em> | <em>CISS Conference</em></li>
          <li>A generative approach to robust machine learning | <em>Jan 2022</em> | <em>RIKEN-AIP TrustML Young Scientist Seminar</em></li>
          <li>Generating novel hard-instances form low-density regions using generative models | <em>Aug 2021</em> | <em>Meta AI</em></li>
          <li>A primer on adversarial machine learning | <em>July 2021</em> | <em>Princeton-Intel REU Seminar</em></li>
          <li>Embedding data distribution to make machine learning more reliable | <em>Mar 2021</em> | <em>EPFL</em></li>
          <li>Private Deep Learning Made Practical | <em>Oct 2019</em> | <em>Qualcomm</em></li>
        </ul>
    </div>

    <div class="news container" id="code">
      <p class="section-heading-small">Academic Services</p>
      <strong>Teaching and Mentoring</strong>
      <div class="pubs-section work-in-progress">
        <ul class="simple-ord-list">
          <li>Lecture on basics of adversarial machine learning | <em>Princeton-Intel REU Seminar 2021</em></li>
          <li>Teaching assistant for ECE 574: Security & Privacy | <em>Fall 2021 - Princeton University</em> </li>
          <li>Taught a mini-course on adversarial attacks & defenses | <em>Winterssion 2020 - Princeton University</em></li>
          <li>Teaching assistant for ELE 535: Machine Learning and Pattern Recognition | <em>Fall 2019 - Princeton University</em></li>
          <li>Mentored ten students in AI research over the years: <em>Edoardo Debenedetti, Rajvardhan Oak, Christian Cianfarani, Tinashe Handina, Matteo Russo, Xianghao Kong, Song Wen, Minzhou Pan, Zhenting Wang, Jie Ren.</em> </li>
        </ul>
      <strong>Other Services</strong>
      <div class="pubs-section work-in-progress">
        <ul class="simple-ord-list">
          <li>Workshop organizer - ICCV 2023 ARROW workshop, CVPR 2023 Workshop of Adversarial Machine Learning on Computer Vision: Art of Robustness 2023</li>
          <li>Program committe member for IEEE Conference on Secure and Trustworthy Machine Learning - 2023</li>
          <li>Organized more than 20 talks on security & privacy in machine learning (<a href="https://vsehwag.github.io/SPML_seminar/">SPML seminar series</a>) - 2022</li>
          <li>One of the core maintainers of Adversarial Robustness Benchmark | <a href="https://robustbench.github.io/"
            target="_blank">robustbench.github.io</a></li>
          <li>Volunteered as junior mentor at Princeton-OLCF-NVIDIA GPU Hackathon | <em>June 2020 - Princeton University</em> </li>
          <li>Reviewed more than 50 papers for major computer vision and machine learning conferences and journals.</em></li>
        </ul>
    </div>
  </div>

  <div class="full-project-divider2">
    <hr />
  </div>

  <footer>
    <small>&copy; 2024, Vikash Sehwag &emsp; &emsp; &emsp; Feel free to use the
      <a href="https://github.com/VSehwag/vsehwag.github.io" , target="_blank">source code.</a>
    </small>
  </footer>
</body>